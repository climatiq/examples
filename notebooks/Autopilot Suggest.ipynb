{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d4d22b29",
      "metadata": {
        "id": "d4d22b29"
      },
      "source": [
        "# Autopilot Suggest Endpoint\n",
        "**Important Notices:**\n",
        "* Please make a **copy** of this file, otherwise other users might be able to access your API key.\n",
        "* Run the individual codes snippets one-by-one and make sure to follow the instructions from top to bottom.\n",
        "* There are several placeholders in the code. Make sure to replace these with the required values.\n",
        "\n",
        "**Functional Overview:**\n",
        "1. Loads structured inputs read from Excel data file.\n",
        "2. Builds and sends payloads to the Climatiq Autopilot Suggest API.\n",
        "3. Handles retries, optional fields, and API errors gracefully.\n",
        "4. Extracts top N emission factor suggestions per row.\n",
        "5. Saves results with detailed metadata back to Excel file.\n",
        "\n",
        "**Installation Requirements:** python3, numpy, pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "ead4666c",
      "metadata": {
        "id": "ead4666c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "495923aa",
      "metadata": {
        "id": "495923aa"
      },
      "source": [
        "## Configuration\n",
        "`file_path` (AP_script_sample_data.xlsx) needs to be replaced with the name of your Excel file of input procurement data.\n",
        "\n",
        "`output_path` (AP_script_output.xlsx) needs to be replaced with the name you would like the output file to be called, which will contain all the matched emission factor data.\n",
        "\n",
        "`max_suggestions` is where the number of desired emission factor matches per item is defined. Currently set to 5, but this can be any value up to 20.\n",
        "\n",
        "`REPLACE_WITH_YOUR_KEY` is where your API key needs to be set. You can retreive a personal API key using [these instructions](https://www.climatiq.io/docs/guides/how-tos/getting-api-key).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "5f19ceda",
      "metadata": {
        "id": "5f19ceda"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "file_path = \"AP_script_sample_data.xlsx\" # Input data file name\n",
        "output_path = \"AP_script_output.xlsx\" # Desired name of the final output file to be called\n",
        "max_suggestions = 5 # Adjust this to as many matches per item as required\n",
        "api_key = \"REPLACE_WITH_YOUR_KEY\"  # Replace with your API key\n",
        "authorization_headers = {\"Authorization\": f\"Bearer {api_key}\"} # Set Authorisation headers with API key\n",
        "url = \"https://preview.api.climatiq.io/autopilot/v1-preview4/suggest\" # URL of the Autopilot Suggest endpoint that the POST request is sent to."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3d23562",
      "metadata": {
        "id": "b3d23562"
      },
      "source": [
        "## HTTP Session Setup\n",
        "Configure retry logic for resilient API calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "6f344d40",
      "metadata": {
        "id": "6f344d40"
      },
      "outputs": [],
      "source": [
        "# Setup session with retry logic\n",
        "session = requests.Session()\n",
        "retries = Retry(connect=3, backoff_factor=0.5)\n",
        "adapter = HTTPAdapter(max_retries=retries)\n",
        "session.mount(\"https://\", adapter)\n",
        "session.mount(\"http://\", adapter)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d22b13bb",
      "metadata": {
        "id": "d22b13bb"
      },
      "source": [
        "## Load and Prepare Data\n",
        "Read and rename input columns to match expected API schema.\n",
        "The columns in your file need to have the same names as the **capitalized names** below.\n",
        "\n",
        "For example, your procured items column name needs to be \"`TEXT`\", unit type needs to be re-named to \"`UNIT_TYPE`\" and so forth.\n",
        "\n",
        "Alternatively, you can change the capitalized column names in the code below to match your existing column header names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "574108eb",
      "metadata": {
        "id": "574108eb"
      },
      "outputs": [],
      "source": [
        "# Load and clean data\n",
        "input_df = pd.read_excel(file_path)\n",
        "input_df = input_df.rename(columns={\n",
        "    'TEXT': 'text',\n",
        "    'MODEL': 'model', # Optional, delete if not used\n",
        "    'UNIT_TYPE': 'unit_type', # Can be Weight, Spend, Volume or Number, one or multiple possible\n",
        "    'YEAR': 'year', # Optional, delete if not used\n",
        "    'REGION': 'region', # Optional, delete if not used\n",
        "    'REGION_FALLBACK': 'region_fallback', # Optional, delete if not used\n",
        "    'SOURCE': 'source', # Optional, delete if not used\n",
        "    'EXCLUDE_SOURCE': 'exclude_source', # Optional, delete if not used\n",
        "    'LCA_ACTIVITY': 'source_lca_activity' # Optional, delete if not used\n",
        "})\n",
        "input_df = input_df.replace(np.nan, '')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc7f5762",
      "metadata": {
        "id": "bc7f5762"
      },
      "source": [
        "## Construct Output Columns\n",
        "Define and preallocate output columns. The `base_cols` array contains the list of column headers that will be populated with the output results. These can be adapted, based on the results you are interested in outputting.\n",
        "\n",
        "If you choose to adapt the outputs, it is then essential to update the code in the `enumerate(results[:max_suggestions])` method in the code in the **\"Iterate and Construct Payloads\"** section of this notebook to align with the new outputs.\n",
        "\n",
        "These `output_cols` will iterate and print as many times as the number of `max_suggestions` specified, to ensure all outputs have column headers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "d446bd10",
      "metadata": {
        "id": "d446bd10"
      },
      "outputs": [],
      "source": [
        "# Define output columns\n",
        "base_cols = ['suggestion_name', 'sector', 'category', 'unit_type', 'source', 'year_relevant', 'year_released',\n",
        "             'region_name', 'source_lca_activity', 'data_quality_flag']\n",
        "output_cols = [f\"{col}_{i+1}\" for i in range(max_suggestions) for col in base_cols]\n",
        "for col in output_cols:\n",
        "    input_df[col] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68f3896a",
      "metadata": {
        "id": "68f3896a"
      },
      "source": [
        "## Iterate and Construct Payloads\n",
        "Below will define the optional input data parameters (`optional_fields` array). The `text` is defined as a required field.\n",
        "The for statement will loop through each row of the input and build payloads for API calls. Handle missing values explicitly.\n",
        "\n",
        "`suggest_payload` will construct the most basic core payload with all the required fields for a given row of data.\n",
        "\n",
        "### Optional Field Handing\n",
        "\n",
        "Inject optional fields if valid and available (not empty).\n",
        "Based on the input type (`year`, `region_fallback` etc), there is type-cast and validation per input field.\n",
        "\n",
        "Parse the cleaned optional inputs to the API payload."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50b3b4b2",
      "metadata": {
        "id": "50b3b4b2",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Optional fields to check for in the input data\n",
        "optional_fields = [\n",
        "    \"model\", \"unit_type\", \"year\", \"region\", \"region_fallback\",\n",
        "    \"source\", \"exclude_source\", \"source_lca_activity\"\n",
        "]\n",
        "\n",
        "# Loop over each row, grab required fields and check for missing data.\n",
        "for idx, row in input_df.iterrows():\n",
        "    text = str(row.get(\"text\", \"\")).strip()\n",
        "\n",
        "    if not text:\n",
        "        for col in output_cols:\n",
        "            input_df.at[idx, col] = \"MISSING_REQUIRED_FIELDS\"\n",
        "        continue\n",
        "\n",
        "    # Construct base payload with 'required' input fields for the Autopilot suggest endpoint.\n",
        "    suggest_payload = {\n",
        "        \"suggest\": {\n",
        "            \"text\": text\n",
        "        },\n",
        "        \"max_suggestions\": max_suggestions\n",
        "    }\n",
        "\n",
        "    # Check each optional field in the current row and skip if empty, otherwise clean the value (remove white spaces).\n",
        "    for field in optional_fields:\n",
        "        value = row.get(field, \"\")\n",
        "        if str(value).strip() == \"\":\n",
        "            continue\n",
        "\n",
        "        clean_value = str(value).strip()\n",
        "\n",
        "        # Type-casting of optional inputs to their correct type\n",
        "        if field == \"year\":\n",
        "            try:\n",
        "                suggest_payload[\"suggest\"][field] = int(clean_value)\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "        elif field == \"region_fallback\":\n",
        "            suggest_payload[\"suggest\"][field] = clean_value.lower() == \"true\"\n",
        "\n",
        "        elif field in [\"unit_type\", \"source\", \"exclude_source\", \"source_lca_activity\"]:\n",
        "\n",
        "            # Ensure mutually exclusive for source/exclude_source\n",
        "            if field == \"exclude_source\" and \"source\" in suggest_payload[\"suggest\"]:\n",
        "                continue\n",
        "            if field == \"source\" and \"exclude_source\" in suggest_payload[\"suggest\"]:\n",
        "                continue\n",
        "\n",
        "            values_array = [v.strip() for v in clean_value.split(\",\") if v.strip()]\n",
        "            if values_array:\n",
        "                suggest_payload[\"suggest\"][field] = values_array\n",
        "\n",
        "        else:\n",
        "            suggest_payload[\"suggest\"][field] = clean_value\n",
        "\n",
        "    # Print constructed payload for each row for debugging\n",
        "    print(f\"\\nPayload for row {idx}:\\n\" + json.dumps(suggest_payload, indent=2))\n",
        "\n",
        "    # Make API request\n",
        "    try:\n",
        "        response = session.post(url, headers=authorization_headers, json=suggest_payload)\n",
        "        response.raise_for_status()\n",
        "        results = response.json().get(\"results\", [])\n",
        "\n",
        "    # Error handling for HTTP request\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"\\nAPI Error at row {idx}: {e}\")\n",
        "        print(f\"Status Code: {response.status_code}\")\n",
        "        print(\"Response Body:\", response.text)\n",
        "\n",
        "        # Clear error output messages\n",
        "        try:\n",
        "            error_data = response.json()\n",
        "            error_code = error_data.get(\"error_code\", \"\")\n",
        "            if error_code == \"no_emission_factors_found\":\n",
        "                for col in output_cols:\n",
        "                    input_df.at[idx, col] = \"NO_MATCH_FOUND\"\n",
        "            else:\n",
        "                for col in output_cols:\n",
        "                    input_df.at[idx, col] = \"API_ERROR\"\n",
        "        except Exception:\n",
        "            for col in output_cols:\n",
        "                input_df.at[idx, col] = \"API_ERROR\"\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"\\nUnexpected error at row {idx}: {e}\")\n",
        "        for col in output_cols:\n",
        "            input_df.at[idx, col] = \"ERROR\"\n",
        "        continue\n",
        "\n",
        "    # Printing the output column headers max_suggestion number of times\n",
        "    # Extract emission factor details from each API suggestion and organize into a data dictionary\n",
        "    for i, suggestion in enumerate(results[:max_suggestions]):\n",
        "        ef = suggestion.get(\"emission_factor\", {})\n",
        "        flags = ef.get(\"data_quality_flags\", [])\n",
        "        data = {\n",
        "            \"suggestion_name\": ef.get(\"name\", \"\"),\n",
        "            \"sector\": ef.get(\"sector\", \"\"),\n",
        "            \"category\": ef.get(\"category\", \"\"),\n",
        "            \"unit_type\": ef.get(\"unit_type\", \"\"),\n",
        "            \"source\": ef.get(\"source\", \"\"),\n",
        "            \"year_relevant\": ef.get(\"year\", \"\"),\n",
        "            \"year_released\": ef.get(\"year_released\", \"\"),\n",
        "            \"region_name\": ef.get(\"region_name\", \"\"),\n",
        "            \"source_lca_activity\": ef.get(\"source_lca_activity\", \"\"),\n",
        "            \"data_quality_flag\": \"TRUE\" if flags else \"FALSE\",\n",
        "            \"suggestion_details\": suggestion.get(\"suggestion_details\", \"\").get(\"label\", \"\")\n",
        "        }\n",
        "\n",
        "        # Write each suggestion's data fields into numbered columns for the current row\n",
        "        for key, val in data.items():\n",
        "            input_df.at[idx, f\"{key}_{i+1}\"] = val\n",
        "\n",
        "    time.sleep(0.2) # Rate limiting delay between API requests"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35a58a67",
      "metadata": {
        "id": "35a58a67"
      },
      "source": [
        "## Export Output\n",
        "Save final suggestions to Excel file with provided name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "f9316a5c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9316a5c",
        "outputId": "24ab9e2c-af25-4281-feed-2efbccff498f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output saved to: AP_script_output.xlsx\n"
          ]
        }
      ],
      "source": [
        "# Save to output Excel file\n",
        "input_df.to_excel(output_path, index=False)\n",
        "print(f\"\\nOutput saved to: {output_path}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}